{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### EPA Air Toxics Screen Data Cleaneing ###\n",
    "\n",
    "# function to read and add 'Year' column\n",
    "def read_and_add_year(file_path, year):\n",
    "    df_cancer = pd.read_csv(file_path + f'/{year}_Cancer_Risk_in_a_million_and_Noncancer_Risk_hazard_quotient.csv')\n",
    "    df_cancer.insert(0, 'Year', year)\n",
    "    return df_cancer\n",
    "\n",
    "# Define file path\n",
    "file_path = 'data/epa-airtoxscreen'\n",
    "\n",
    "# Read and add 'Year' column\n",
    "cancer_2014 = read_and_add_year(file_path, 2014)\n",
    "cancer_2017 = read_and_add_year(file_path, 2017)\n",
    "cancer_2018 = read_and_add_year(file_path, 2018)\n",
    "cancer_2019 = read_and_add_year(file_path, 2019)\n",
    "\n",
    "# function to remove rows with total cancer risk less than 0.01\n",
    "def cancer_rows(*cancer_dfs):\n",
    "    return [df[df['Total Cancer Risk (per million)'] >= 1] for df in cancer_dfs]\n",
    "\n",
    "# Remove rows\n",
    "cancer_2014, cancer_2017, cancer_2018, cancer_2019 = cancer_rows(cancer_2014, cancer_2017, cancer_2018, cancer_2019)\n",
    "\n",
    "# Concatenate DataFrames for all years\n",
    "epa_combined = pd.concat([cancer_2014, cancer_2017, cancer_2018, cancer_2019], ignore_index=True)\n",
    "\n",
    "# format columns\n",
    "epa_combined = epa_combined.drop(columns=['EPA Region','FIPS'])\n",
    "epa_combined = epa_combined.rename(columns={'Tract': 'CensusTract'})\n",
    "epa_combined = epa_combined[epa_combined['County'] != 'Entire State']\n",
    "epa_combined = epa_combined.loc[:, epa_combined.sum() != 0]\n",
    "\n",
    "# Concatenate pollutant names and sum the numeric columns\n",
    "epa_combined = epa_combined.groupby(['CensusTract', 'Year']).agg(\n",
    "    {\n",
    "        'State': 'first',\n",
    "        'County': 'first',\n",
    "        'Pollutant Name': ' | '.join,  # Concatenate pollutant names separated by ' | '\n",
    "        **{col: 'sum' for col in epa_combined.columns if epa_combined[col].dtype in ['float64', 'int64'] and col not in ['CensusTract', 'Year', 'State', 'County']}\n",
    "    }\n",
    ").reset_index()\n",
    "\n",
    "# Since Population is repeated within the same CensusTract and Year adjust it to avoid overcounting\n",
    "epa_combined['Population'] = epa_combined.groupby(['CensusTract', 'Year'])['Population'].mean().values\n",
    "\n",
    "# reorder columns\n",
    "epa_combined = epa_combined[['CensusTract', 'Year', 'State', 'County', 'Population'] + [col for col in epa_combined.columns if col not in ['CensusTract', 'Year', 'State', 'County', 'Population']]]\n",
    "\n",
    "# print to csv\n",
    "epa_combined.to_csv('data/epa-airtoxscreen/epa_combined.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Social Vulnerability Index Data Cleaning ###\n",
    "\n",
    "# read data\n",
    "social_2016 = pd.read_csv('data/social-vulnerability-index/Utah-2016.csv')\n",
    "social_2018 = pd.read_csv('data/social-vulnerability-index/Utah-2018.csv')\n",
    "social_2020 = pd.read_csv('data/social-vulnerability-index/Utah-2020.csv')\n",
    "\n",
    "# add 'Year' column\n",
    "social_2016.insert(0, 'Year', 2016)\n",
    "social_2018.insert(0, 'Year', 2018)\n",
    "social_2020.insert(0, 'Year', 2020)\n",
    "\n",
    "# Concatenate DataFrames for all years\n",
    "svi_combined = pd.concat([social_2016, social_2018, social_2020], ignore_index=True)\n",
    "\n",
    "# format columns\n",
    "svi_combined = svi_combined.drop(columns=['ST','STATE','STCNTY',\"LOCATION\"])\n",
    "svi_combined = svi_combined.rename(columns={'FIPS':'CensusTract','COUNTY':'County','ST_ABBR':'State'})\n",
    "\n",
    "# reorder columns\n",
    "svi_combined = svi_combined[['CensusTract', 'Year', 'State', 'County'] + [col for col in svi_combined.columns if col not in ['CensusTract', 'Year', 'State', 'County']]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Food access data cleaning ###\n",
    "\n",
    "# read data\n",
    "food_2010 = pd.read_csv('data/USDA_food_access/food_access_2010.csv')\n",
    "food_2015 = pd.read_csv('data/USDA_food_access/food_access_2015.csv')\n",
    "food_2019 = pd.read_csv('data/USDA_food_access/food_access_2019.csv')\n",
    "\n",
    "# add 'Year' column\n",
    "food_2010.insert(0, 'Year', 2010)\n",
    "food_2015.insert(0, 'Year', 2015)\n",
    "food_2019.insert(0, 'Year', 2019)\n",
    "\n",
    "# Concatenate DataFrames for all years\n",
    "food_combined = pd.concat([food_2010, food_2015, food_2019], ignore_index=True)\n",
    "\n",
    "# reorder columns\n",
    "food_combined = food_combined[['CensusTract', 'Year', 'State', 'County'] + [col for col in food_combined.columns if col not in ['CensusTract', 'Year', 'State', 'County']]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duplicates: False\n"
     ]
    }
   ],
   "source": [
    "# Merge all data\n",
    "merged_data = pd.merge(epa_combined, svi_combined, on=['CensusTract','Year','State','County'], how='outer')\n",
    "merged_data = pd.merge(merged_data, food_combined, on=['CensusTract','Year','State','County'], how='outer')\n",
    "merged_data = merged_data.drop_duplicates()\n",
    "merged_data['County'] = merged_data['County'].str.replace(' County', '', case=False)\n",
    "\n",
    "# Group by 'CensusTract' and 'Year', then fill NaN values within each group and drop duplicates\n",
    "grouped_new = merged_data.groupby(['CensusTract', 'Year'], as_index=False)\n",
    "merged_data = grouped_new.apply(lambda x: x.ffill().bfill()).drop_duplicates(subset=['CensusTract', 'Year'])\n",
    "merged_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# check for duplicates\n",
    "dup_check = merged_data.duplicated(subset=['CensusTract', 'Year'], keep=False)\n",
    "\n",
    "# Display whether any duplicates were found\n",
    "print(\"duplicates: \" + str(dup_check.any()))\n",
    "\n",
    "# print to csv\n",
    "merged_data.to_csv('data/merged_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duplicates: False\n"
     ]
    }
   ],
   "source": [
    "# check for duplicates\n",
    "dup_check = condensed_data_new.duplicated(subset=['CensusTract', 'Year'], keep=False)\n",
    "\n",
    "# Display whether any duplicates were found\n",
    "print(\"duplicates: \" + str(dup_check.any()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
